{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for general use\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# for feature selection\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# for feature selection testing\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import xgboost\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "# for hyperparameter finetuning\n",
    "import itertools\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# for model explaination\n",
    "import sys\n",
    "!{sys.executable} -m pip install shap\n",
    "import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to adjust the path 'dataset/dataset.shp' if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the dataset\n",
    "dataset = gpd.read_file('dataset/dataset.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get only rows with AGB values and drop geometry\n",
    "modeling_dataset = dataset.drop('geometry', 1)\n",
    "modeling_dataset = pd.DataFrame(modeling_dataset.loc[dataset['AGB_t_ha']!=-9999.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate modeling_dataset into features (X) and AGB (y)\n",
    "X = modeling_dataset.drop('AGB_t_ha',1)\n",
    "y = pd.DataFrame(modeling_dataset['AGB_t_ha'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a unique identifier for each forestry plot\n",
    "# the default dataset contains a total of 73 forestry plots\n",
    "plot_df = pd.DataFrame()\n",
    "plot_df['plot_number']=np.argwhere(y.values == np.unique(y))[:,1]\n",
    "# adjust index\n",
    "plot_df.index=y.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define function: 7-folds cross-valalidation and normalization with RobustScaler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the purpose of generating reliable results, we now built a function which performs data normalization with the RobustScaler and a 7-folds cross-validation by posing 2 constraints within the function. \n",
    "- Firstly, points belonging to the same original forestry plot can be included exclusively in the training or in the validation set; \n",
    "- Secondly, only the training set is used to fit the scaler, this allows validation data to remain unseen throughout the process. The entire dataset is finally normalized utilizing the trained scaler. \n",
    "\n",
    "This function is used before each step, that is feature selection, feature selection testing, and hyper-parameters fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val(X,y,folds=7,random_state=42):\n",
    "    \n",
    "    ''' \n",
    "    cross_val function sets 2 contraints:\n",
    "    - points belonging to the same plot (plot_df) end up in either the training or the validation set;\n",
    "    - only the training set fits the scaler.\n",
    "    it outputs training and validation set\n",
    "    \n",
    "               REQUIRES\n",
    "               plot_df!\n",
    "    \n",
    "    plot_df: forestry plot unique identifier\n",
    "    X: independent variables (Vegetation Indices, texture measures, etc.)\n",
    "    y: target variable (AGB)\n",
    "      \n",
    "    '''\n",
    "      \n",
    "    kf = KFold(n_splits=folds)\n",
    "    \n",
    "    # interates over number of train_plots and val_plots for each fold\n",
    "    for train_plots, val_plots in kf.split(np.unique(plot_df['plot_number'].unique())): \n",
    "        \n",
    "        # goes to X, checks which entries have plot number in the train_plots and stacks the entries\n",
    "        X_train =np.vstack([X[plot_df['plot_number']==plot].values for plot in train_plots])       \n",
    "        # goes to X, checks which entries have plot number in the train_plots and stacks the entries\n",
    "        X_val =np.vstack([X[plot_df['plot_number']==plot].values for plot in val_plots])\n",
    "        \n",
    "        # goes to y, checks which entries have plot number in the train_plots and stacks the entries\n",
    "        y_train =pd.DataFrame(np.vstack([y[plot_df['plot_number']==plot].values for plot in train_plots]))\n",
    "        y_train.columns=y.columns         \n",
    "        # goes to y, checks which entries have plot number in the train_plots and stacks the entries\n",
    "        y_val =pd.DataFrame(np.vstack([y[plot_df['plot_number']==plot].values for plot in val_plots]))\n",
    "        y_train.columns=y.columns \n",
    "        \n",
    "        # scale with RobustScaler\n",
    "        scaler = preprocessing.RobustScaler().fit(X_train)\n",
    "        X_train = pd.DataFrame(scaler.transform(X_train))\n",
    "        X_train.columns=X.columns        \n",
    "        X_val = pd.DataFrame(scaler.transform(X_val))\n",
    "        X_val.columns=X.columns    \n",
    "        yield (X_train,y_train),(X_val,y_val) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature selection\n",
    "**note:** the default dataset has 63 features. Adjust the code if you are working with different data.\n",
    "\n",
    "In order to build a reliable model for the estimation of Above Ground Biomass (AGB), careful evaluation is required when deciding which and how many features to include in the predictive model. A total of 4 features selection methods are evaluated, including both supervised and unsupervised approaches. Other than the Mean Decrease in Impurity (MDI) and the Mean Decrease in Accuracy (MDA), this work explores the potential of L1 Regularization (LASSO); both of which are supervised methods - i.e. the target is known. Hence, an unsupervised method, Principal Component Analysis (PCA) is included, and its performance evaluated.\n",
    "\n",
    "### 3.1. Feature selection w/ Random Forest Regressor (Mean Decrease in Impurity)\n",
    "Impurity measures can be used for feature selection by evaluating the extent to which each feature contributes to decreasing the averaged impurity in each tree composing the forest, so as to calculate the MDI for each feature. The feature able to account for more variance decrease is going to be at the top of the ranking. Therefore, the MDI can be see as the total decrease in node impurity from splitting on the variable, averaged over all trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# inititate an empty list\n",
    "feature_imps_MDI=[]\n",
    "\n",
    "for train,val in cross_val(X,y,folds=7,random_state=42):    \n",
    "    \n",
    "    # create a Random Forest model\n",
    "    rf_model = RandomForestRegressor()   \n",
    "    # fit on training data (0 is X_train, 1 is y_train)\n",
    "    rf_model.fit(train[0], train[1].values.ravel())    \n",
    "    # store feature importances of the folds in a list\n",
    "    feature_imps_MDI.append(rf_model.feature_importances_)\n",
    "\n",
    "# average of values over all folds\n",
    "mean_values_MDI = np.mean(feature_imps_MDI,axis=0)\n",
    "\n",
    "#save indexes (column order) for later\n",
    "index_MDI = mean_values_MDI.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot average feature importance of all folds\n",
    "RF_feat_importance = pd.Series(mean_values_MDI, index=X.columns)\n",
    "RF_feat_importance.nsmallest(63).plot(kind='barh', figsize=(5,15), color=\"#40d491ff\")\n",
    "plt.title('Features importance by Mean Decrease in Impurity (MDI)')\n",
    "plt.xlabel('Mean Decrease in Impurity')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Feature selection w/ Random Forest Regressor (Mean Decrease in Accuracy)\n",
    "Mean Decrease in Accuracy (MDA) performs a ranking of features based on the increasing mean error of a tree in the forest when the feature values are randomly shuffled though maintaining their distribution. For regression problems, the error refers to the Mean Squared Error (MSE). MDA is performed by measuring the impact of each feature on the model MSE by starting from a MSE\n",
    "baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate an empty list\n",
    "feature_imps_MDA=[]\n",
    "\n",
    "for train,val in cross_val(X,y,folds=7,random_state=42):\n",
    "    \n",
    "    # create a Random Forest model\n",
    "    rf_model = RandomForestRegressor()\n",
    "    # fit on training data (0 is X_train, 1 is y_train)\n",
    "    rf_model.fit(train[0], train[1].values.ravel())\n",
    "    # store feature importances of the folds in a list\n",
    "    feature_imps_MDA.append(permutation_importance(rf_model, train[0], train[1]))\n",
    "\n",
    "\n",
    "importances_mean_MDA = []\n",
    "for index in range(len(feature_imps_MDA)):\n",
    "    importances_mean_MDA.append(feature_imps_MDA[index]['importances_mean'])\n",
    "    \n",
    "# average values over all folds\n",
    "mean_values_MDA = np.mean(importances_mean_MDA, axis=0)\n",
    "\n",
    "# save indices (column order) for later\n",
    "index_MDA = mean_values_MDA.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot average feature imporance of all the folds\n",
    "MDA_feat_importance = pd.Series(mean_values_MDA, index=X.columns)\n",
    "MDA_feat_importance.nsmallest(63).plot(kind='barh', figsize=(5,15), color=\"#40d491ff\")\n",
    "plt.title('Features importance by Mean Decrease in Accuracy (MDA)')\n",
    "plt.tight_layout()\n",
    "plt.figure(figsize=(4,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Feature selection w/ Principal Component Analysis\n",
    "How is it possible to do feature selection with PCA? We can use the loadings. What are the loeadings, you ask? Its the weight that each variable has for a components of PCA. Hence, a variable with higher loading means that it is more important for the explaining the variance of the data. Let's see what happens if we use **only the first principal component**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inititate an empty list\n",
    "Loadings_PC1=[]\n",
    "\n",
    "for train,val in cross_val(X,y,folds=7,random_state=42):\n",
    "    \n",
    "    # create a pca model\n",
    "    PCA_model = PCA(n_components=1).fit(train[0])\n",
    "    Loadings_PC1.append(PCA_model.components_)\n",
    "\n",
    "# average of values over all folds\n",
    "mean_loadings_PCA = np.mean(Loadings_PC1,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loading values for the first principal component\n",
    "plt.figure(figsize=(4,15))\n",
    "index = np.argsort(np.abs(mean_loadings_PCA[0]))\n",
    "plt.barh(X.columns[index],np.sort(np.abs(mean_loadings_PCA[0])),color=\"#40d491ff\")\n",
    "plt.title('Loading Values for the first Principal Component')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ...but what about the other components? \n",
    "Thats a good question! In fact, the first component only accounts for a percentage of the total variation of the data. Let us check the loadings of our variables in the **first and second components**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inititate an empty list\n",
    "Loadings_PC1=[]\n",
    "Loadings_PC2=[]\n",
    "\n",
    "for train,val in cross_val(X,y,folds=7,random_state=42):\n",
    "    \n",
    "    # create a PCA model\n",
    "    PCA_model = PCA(n_components=2).fit(train[0])\n",
    "    Loadings_PC1.append( PCA_model.components_[0])\n",
    "    Loadings_PC2.append( PCA_model.components_[1])\n",
    "\n",
    "\n",
    "# average of values over all folds\n",
    "mean_loadings_PC1 = np.mean(Loadings_PC1,axis=0)\n",
    "mean_loadings_PC2 = np.mean(Loadings_PC2,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loading values for the first and second principal components\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8,12))\n",
    "index = np.argsort(np.abs(mean_loadings_PC1))\n",
    "# generate first plot\n",
    "ax1.barh(X.columns[index],np.abs(mean_loadings_PC1)[index],color=\"#40d491ff\")\n",
    "ax1.set_title(f'First Principal Component {np.round(100*PCA_model.explained_variance_ratio_[0],1)}%')\n",
    "# generate second plot\n",
    "ax2.barh(X.columns[index],np.abs(mean_loadings_PC2)[index],color=\"#40d491ff\")\n",
    "ax2.set_title(f'Second Principal Component {np.round(100*PCA_model.explained_variance_ratio_[1],1)}%')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### That's cool and all but, how do we do the feature selection then?\n",
    "We are going to do a weighted sum over a sufficent number of principal components. We sum the loading and weight it with the explained variance ratio of each componet.\n",
    "\n",
    "##### But what is a sufficient number of components?\n",
    "Thats a good question. Eventually the total explained variance starts to converge as we add more components. We are now going to test that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inititate an empty list\n",
    "Loadings=[]\n",
    "\n",
    "for train,val in cross_val(X,y,folds=7,random_state=42):\n",
    "    \n",
    "    # create a PCA model\n",
    "    # you can experiment with different number of components\n",
    "    PCA_model = PCA(n_components=25).fit(train[0])\n",
    "    Loadings.append(PCA_model.components_[0])\n",
    "\n",
    "# average of values over all folds\n",
    "mean_loadings = np.mean(Loadings,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the cumulative explained variance\n",
    "plt.bar(np.arange(25)[:11],np.cumsum(PCA_model.explained_variance_ratio_)[:11],color=\"#40d491ff\",alpha=0.9)\n",
    "plt.bar(np.arange(25)[11:],np.cumsum(PCA_model.explained_variance_ratio_)[11:],alpha=0.3,color=\"#40d491ff\")\n",
    "plt.hlines(0.95,xmin=0, xmax=25, linestyle='--', color='#ff7f4dff')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's know check whether the order of the top ranked features changes when using 5, 10 and 20 components. We first need to create a function which calculates the weight of the components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def molisse_method(n_components, X):\n",
    "    \n",
    "    '''\n",
    "    molisse_method finds the weight (i.e. dataset variance explained) of each component to after perform feature selection\n",
    "    \n",
    "    n_components:number of principal components\n",
    "    X: dataset variables\n",
    "    \n",
    "    '''\n",
    "    pca_model = PCA(n_components=n_components).fit(X)\n",
    "    weight = np.zeros(shape=(X.shape[1]))\n",
    "    for component in range(n_components):\n",
    "        weight += np.abs(pca_model.components_[component])*pca_model.explained_variance_[component]\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inititate empty list\n",
    "Loadings_5 =[]\n",
    "Loadings_10 =[]\n",
    "Loadings_20 =[]\n",
    "weights_5 = []\n",
    "weights_10 = []\n",
    "weights_20 = []\n",
    "\n",
    "for train,val in cross_val(X,y,folds=7,random_state=42):\n",
    "    \n",
    "    weights_5.append(molisse_method(5,train[0]))\n",
    "    weights_10.append(molisse_method(10,train[0]))\n",
    "    weights_20.append(molisse_method(20,train[0]))\n",
    "    \n",
    "    PCA_model_5 = PCA(n_components=5).fit(train[0])\n",
    "    PCA_model_10 = PCA(n_components=10).fit(train[0])\n",
    "    PCA_model_20 = PCA(n_components=20).fit(train[0])\n",
    "    \n",
    "    Loadings_5.append(PCA_model_5.components_[0])\n",
    "    Loadings_10.append(PCA_model_10.components_[0])\n",
    "    Loadings_20.append(PCA_model_20.components_[0])\n",
    "\n",
    "# mean of values over all folds\n",
    "mean_weights_5 = np.mean(weights_5, axis=0)\n",
    "mean_weights_10 = np.mean(weights_10, axis=0)\n",
    "mean_weights_20 = np.mean(weights_20, axis=0)\n",
    "\n",
    "mean_loadings_5 = np.mean(Loadings_5,axis=0)\n",
    "mean_loadings_10 = np.mean(Loadings_10,axis=0)\n",
    "mean_loadings_20 = np.mean(Loadings_20,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot weighted loading values for the 5, 10 and 20 components\n",
    "fig, (ax1, ax2,ax3) = plt.subplots(1, 3, figsize=(12,12))\n",
    "\n",
    "\n",
    "index_weights_5 = np.argsort(mean_weights_5)\n",
    "ax1.barh(X.columns[index_weights_5],mean_weights_5[index_weights_5],color=\"#40d491ff\")\n",
    "ax1.set_title(f'5 Principal Components')\n",
    "\n",
    "index_weights_10 = np.argsort(mean_weights_10)\n",
    "ax2.barh(X.columns[index_weights_10],mean_weights_10[index_weights_10],color=\"#40d491ff\")\n",
    "ax2.set_title(f'10 Principal Components')\n",
    "\n",
    "index_weights_20 = np.argsort(mean_weights_20)\n",
    "ax3.barh(X.columns[index_weights_20],mean_weights_10[index_weights_20],color=\"#40d491ff\")\n",
    "ax3.set_title(f'20 Principal Components')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# we pick the result from 10 components\n",
    "# save 10 components indices (column order) for later\n",
    "index_PCA_10 = index_weights_10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Feature selection w/ L1 regularization (LASSO)\n",
    "Regularization algorithms aim to minimize the residual sum of squares of the model by using the Ordinary Least Square (OLS); this is achieved by shrinking the estimated regression coefficients approaching to zero. The Least Absolute Shrinkage and Selection Operator (LASSO) was proposed by Tibshirani (1996); LASSO regression is an L1 Regularization technique, which minimizes the absolute sum of the coefficients; while L2 regularization minimizes the squared sum of the coefficients.\n",
    "\n",
    "Feature selection with LASSO is performed by multiplying every coefficient by a constant, that is the regularization parameter alpha (α). Alpha controls the strength of penalty, meaning that for large values of alpha, a large number of coefficients is forced to zero which corresponds to the exclusion of those features from the model; on the other hand, when alpha is equal to zero, we have the classic OLS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell is going to give some warnings asking to increase the number of iterations. feel free to adapt it.\n",
    "# initialize empty list\n",
    "count = []\n",
    "\n",
    "for train,val in cross_val(X,y,folds=7,random_state=42):\n",
    "    count.append([np.sum(Lasso(alpha=cost,max_iter=20000).fit(train[0],train[1]).coef_!=0).astype(int) for cost in np.linspace(0,10,1000)[::-1]])\n",
    "\n",
    "# average of values over all folds\n",
    "mean_count = np.mean(count, axis=0, dtype=np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot number of selected features vs regularization parameter values\n",
    "plt.plot(np.linspace(0,10,1000)[::-1],mean_count,color=\"#40d491ff\")\n",
    "plt.xlabel('Regularization parameter')\n",
    "plt.ylabel('Number of features selected')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_selection_f = []\n",
    "weights_selection_f = []\n",
    "\n",
    "binary_selection=np.zeros(shape=(63))\n",
    "weights_selection = np.zeros(shape=(63))\n",
    "\n",
    "    \n",
    "for train,val in cross_val(X,y,folds=7,random_state=42):\n",
    "    \n",
    "    for cost in np.linspace(0,10,1000)[::-1]:\n",
    "        selector = Lasso(alpha=cost,max_iter=5000).fit(train[0],train[1])\n",
    "        binary_selection += (selector.coef_!=0).astype(int)\n",
    "        weights_selection += np.abs(selector.coef_)\n",
    "        \n",
    "    binary_selection_f.append(binary_selection)\n",
    "    weights_selection_f.append(weights_selection)\n",
    "\n",
    "\n",
    "mean_binary_selection=np.mean(binary_selection_f, axis=0)\n",
    "mean_weights_selection=np.mean(weights_selection_f, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot feature importance\n",
    "\n",
    "# this plot adds up to 7000 because a range from 0 to 10, with a total of 1000 values, was chose for alpha;\n",
    "# this was looped over using 7-fold crossvalidation -> 7000\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,15))\n",
    "index_lasso = np.argsort(mean_binary_selection)\n",
    "ax1.barh(X.columns[index_lasso],mean_binary_selection[index_lasso], color=\"#40d491ff\")\n",
    "ax1.set_title('L1 Selection Ranking')\n",
    "ax1.set_xlabel('Frequency of selection')\n",
    "\n",
    "ax2.barh(X.columns[index_lasso],mean_weights_selection[index_lasso], color=\"#40d491ff\")\n",
    "ax2.set_title('Total Sum of weights')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature selection testing & hyper-parameters optimization\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cells, the previously explained feature selection methods are tested and evaluated using several non-parametric Machine Learning (ML) algorithms: K-Nearest Neighbour (kNN), Random Forest (RF), Extreme Gradient Boosting (XGB) and, lastly, 3 Artificial Neural Networks (ANN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list with all the feature selection rankings to be tested\n",
    "index_list = [index_MDI,index_MDA,index_PCA_10,index_lasso]\n",
    "# create a list with feature selection methods names\n",
    "names = ['MDI','MDA','PCA','LASSO']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. k-Nearest Neighbour\n",
    "\n",
    "### 4.1.1. k-Nearest Neighbour: feature selection testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_results=[]\n",
    "for train, val in cross_val(X,y,folds=7,random_state=42):\n",
    "    results=[]\n",
    "    for index in index_list:\n",
    "        selected_features = []\n",
    "        method_results=[]\n",
    "        for feature in train[0].columns[index[::-1]]:\n",
    "            selected_features.append(feature) \n",
    "            \n",
    "            model = KNeighborsRegressor().fit(train[0][selected_features], train[1].values.flatten())\n",
    "            KNN_preds = model.predict(val[0][selected_features])\n",
    "            rmse = np.sqrt(np.mean(np.square(KNN_preds-val[1].values.flatten())))\n",
    "            \n",
    "            method_results.append(rmse)\n",
    "        results.append(method_results)\n",
    "    fold_results.append(results)\n",
    "\n",
    "\n",
    "results = np.mean(fold_results,axis=0)\n",
    "error = np.std(fold_results,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2)=plt.subplots(1,2,figsize=(13,5))\n",
    "\n",
    "winner = np.argwhere(results == np.min(results)).flatten()[0]\n",
    "\n",
    "for i in range(4):\n",
    "    colors = [\"#40d491ff\",\"#ffe24dff\",\"#ff7f4dff\",\"#397abdff\"]\n",
    "    ax1.plot(results[i],label=names[i], color=colors[i])\n",
    "ax1.hlines(np.min(results),xmin=0,xmax=63,linestyle='--',color='#25521988',label='min RMSE')\n",
    "ax1.set_title(f'Regressor: kNN baseline \\nMinimum RMSE: {np.round(np.min(results),2)} t/ha \\n Method: {names[winner]}')\n",
    "ax1.set_ylabel('RMSE (t/ha)')\n",
    "ax1.set_xlabel('Number of features')\n",
    "\n",
    "ax1.legend()\n",
    "\n",
    "for i in range(4):\n",
    "    colors = [\"#40d491ff\",\"#ffe24dff\",\"#ff7f4dff\",\"#397abdff\"]\n",
    "    ax2.plot(results[i],label=names[i], color=colors[i])\n",
    "ax2.hlines(np.min(results),xmin=0,xmax=63,linestyle='--',color='#25521988',label='Min RMSE')\n",
    "ax2.set_title(f'Regressor: kNN baseline \\nMinimum RMSE: {np.round(np.min(results),2)} t/ha \\n Method: {names[winner]}')\n",
    "ax2.set_xlim([19,26])\n",
    "ax2.set_ylim([43.5,46])\n",
    "ax2.set_ylabel('RMSE (t/ha)')\n",
    "ax2.set_xlabel('Number of features')\n",
    "\n",
    "ax2.legend()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **IMPORTANT :**   \n",
    "Create a list of the best performing features. Such that, if the best perfoming k-NN model was achieved by using the first 15 features selected by the MDA feature selection method, create a list with such fueatures.\n",
    "\n",
    "i.e. add a cell such that\n",
    "\n",
    "kNN_X = X [ [ ' feature1 ' , ' feature2 ' , ' feature3 ' , ... , ' featureN ' ] ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2. k-Nearest Neighbour: hyper-parameters optimization (Grid Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: kNN_X is the list of features you created in the previous step\n",
    "# you need to define kNN_X\n",
    "\n",
    "# list Hyperparameters that we want to tune.\n",
    "n_neighbors = (range(1,51))\n",
    "# convert to dictionary\n",
    "kNN_hyperparameters = dict(n_neighbors=n_neighbors)\n",
    "# scoring matrix\n",
    "scoring = [\"neg_mean_squared_error\"]\n",
    "\n",
    "# initialize empty list\n",
    "neighbor_loss = []\n",
    "\n",
    "for n_neighbors in np.arange(1,50):\n",
    "    rmse=[]\n",
    "    for train, val in cross_val(kNN_X,y,folds=7):\n",
    "        model = KNeighborsRegressor(n_neighbors=n_neighbors)\n",
    "        model.fit(train[0],train[1].values.flatten())\n",
    "        preds = model.predict(val[0])\n",
    "        rmse.append(np.sqrt(np.mean(np.square(preds-val[1].values.flatten()))))\n",
    "          \n",
    "    neighbor_loss.append(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1,50), np.mean(neighbor_loss, axis=1))\n",
    "plt.hlines(np.min(np.mean(neighbor_loss, axis=1)),xmin=0,xmax=63,linestyle='--',color='#25521988',label='Min RMSE')\n",
    "plt.title(f'kNN w/ GridSearch \\nMinimum RMSE: {np.round(np.min(np.mean(neighbor_loss, axis=1)), 2)} t/ha, \\nNumber of neighbours: 11')\n",
    "plt.xlabel(\"Number of neighbours\")\n",
    "plt.ylabel(\"RMSE (t/ha)\")\n",
    "plt.xlim([0,50])\n",
    "plt.ylim([42,50])\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Random Forest\n",
    "### 4.2.1. Random Forest: feature selection testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_results=[]\n",
    "for train, val in cross_val(X,y,folds=7,random_state=42):\n",
    "    results=[]\n",
    "    for index in index_list:\n",
    "        selected_features = []\n",
    "        method_results=[]\n",
    "        for feature in train[0].columns[index[::-1]]:\n",
    "            selected_features.append(feature)\n",
    "\n",
    "            model  = RandomForestRegressor(random_state=42).fit(train[0][selected_features], train[1].values.flatten())\n",
    "            RF_preds = model.predict(val[0][selected_features])\n",
    "            rmse = np.sqrt(np.mean(np.square(RF_preds-val[1].values.flatten())))\n",
    "\n",
    "            method_results.append(rmse)\n",
    "        results.append(method_results)\n",
    "    fold_results.append(results)\n",
    "    \n",
    "results = np.mean(fold_results,axis=0)\n",
    "error = np.std(fold_results,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the feature selection testing w/ RF\n",
    "fig,(ax1,ax2)=plt.subplots(1,2,figsize=(13,5))\n",
    "\n",
    "winner = np.argwhere(results == np.min(results)).flatten()[0]\n",
    "\n",
    "for i in range(4):\n",
    "    colors = [\"#40d491ff\",\"#ffe24dff\",\"#ff7f4dff\",\"#397abdff\"]\n",
    "    #ax1.fill_between(np.arange(63), results[i]-error[i], results[i]+error[i],alpha=0.15, color=colors[i])\n",
    "    ax1.plot(results[i],label=names[i], color=colors[i])\n",
    "ax1.hlines(np.min(results),xmin=0,xmax=63,linestyle='--',color='#25521988',label='min RMSE')\n",
    "ax1.set_title(f'Random Forest (baseline) \\nMinimum RMSE: {np.round(np.min(results),2)}t/ha \\n Method: {names[winner]}')\n",
    "ax1.set_ylabel('RMSE (t/ha)')\n",
    "ax1.set_xlabel('Number of features')\n",
    "#plt.ylabel(\"Root Mean Squared Error\")\n",
    "#ax1.text(63, np.round(np.min(results)), 'Minimum')\n",
    "\n",
    "ax1.legend(loc='upper right')\n",
    "\n",
    "for i in range(4):\n",
    "    colors = [\"#40d491ff\",\"#ffe24dff\",\"#ff7f4dff\",\"#397abdff\"]\n",
    "    #ax2.fill_between(np.arange(63), results[i]-error[i], results[i]+error[i],alpha=0.15, color=colors[i])\n",
    "    ax2.plot(results[i],label=names[i], color=colors[i])\n",
    "ax2.hlines(np.min(results),xmin=0,xmax=63,linestyle='--',color='#25521988',label='min RMSE')\n",
    "ax2.set_title(f'Random Forest (baseline) \\nMinimum RMSE: {np.round(np.min(results),2)}t/ha \\n Method: {names[winner]}')\n",
    "ax2.set_ylabel('RMSE (t/ha)')\n",
    "ax2.set_xlabel('Number of features')\n",
    "ax2.set_xlim([2,8])\n",
    "ax2.set_ylim([41,60])\n",
    "#plt.xlabel(\"Number of included features\")\n",
    "\n",
    "#ax2.text(2, np.round(np.min(results)), 'Minimum')\n",
    "ax2.legend(loc='upper right')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **IMPORTANT :**   \n",
    "Create a list of the best performing features. Such that, if the best perfoming RF model was achieved by using the first 15 features selected by the MDA feature selection method, create a list with such fueatures.\n",
    "\n",
    "i.e. add a cell such that\n",
    "\n",
    "rf_X = X [ [ ' feature1 ' , ' feature2 ' , ' feature3 ' , ... , ' featureN ' ] ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2. Random Forest: hyper-parameter optimization (Grid Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: rf_X is the list of features you created in the previous step\n",
    "# you need to define rf_X\n",
    "\n",
    "grid_best_loss = 1000\n",
    "grid_parameter_loss=[]\n",
    "# define a list with hyperparameters to be tested\n",
    "for n_estimators,max_depth,min_samples_split, min_samples_leaf in list(itertools.product(np.arange(30,1000),\n",
    "                                                                                        np.arange(10,100),\n",
    "                                                                                        np.arange(2,5),\n",
    "                                                                                       np.arange(1,2))):\n",
    "    rmse=[]\n",
    "    for train, val in cross_val(rf_X,y,folds=7,random_state=42):\n",
    "        model = RandomForestRegressor(random_state=42, n_estimators=n_estimators,\n",
    "                                      max_depth = max_depth,\n",
    "                                      min_samples_split=min_samples_split,\n",
    "                                      min_samples_leaf=min_samples_leaf)\n",
    "        model.fit(train[0],train[1].values.flatten())\n",
    "        preds = model.predict(val[0])\n",
    "        rmse.append(np.sqrt(np.mean(np.square(preds-val[1].values.flatten()))))\n",
    "    \n",
    "    if np.mean(rmse) < grid_best_loss:\n",
    "        grid_best_parameters = [n_estimators, max_depth, min_samples_split, min_samples_leaf]\n",
    "        grid_best_loss = np.mean(rmse)\n",
    "    grid_parameter_loss.append(np.mean(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the results of grid search\n",
    "print (\"Random Forest with GridSearch\",\"\\n_\",\"\\nRMSE:\", grid_best_loss ,\"\\n_\",\"\\nn_estimators:\", grid_best_parameters[0], \n",
    "       \"\\nmax_depth:\", grid_best_parameters[1],\"\\nmin_samples_split:\", grid_best_parameters[2], \n",
    "       \"\\nmin_samples_leaf:\", grid_best_parameters[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3. Random Forest: hyper-parameter optimization (Random Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a dictionary with hyperparameters to be tested\n",
    "random_hyperparameters = dict(n_estimators=np.arange(30,1000,10),\n",
    "                        max_depth=np.arange(10,100,10),\n",
    "                        min_samples_split = np.arange(2,5,10),\n",
    "                        min_samples_leaf=np.arange(1,2,4),\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: rf_X is the list of features you created in the previous step\n",
    "# you need to define rf_X\n",
    "\n",
    "n_iterations = 100\n",
    "best_loss = 1000\n",
    "for _ in range(n_iterations):\n",
    "    n_estimators = np.random.choice(random_hyperparameters['n_estimators'],replace=True)\n",
    "    max_depth=np.random.choice(random_hyperparameters['max_depth'],replace=True)\n",
    "    min_samples_split = np.random.choice(random_hyperparameters['min_samples_split'],replace=True)\n",
    "    min_samples_leaf=np.random.choice(random_hyperparameters['min_samples_leaf'],replace=True)\n",
    "    \n",
    "    rmse = []\n",
    "    for train, val in cross_val(rf_X,y,folds=7,random_state=42):\n",
    "        model = RandomForestRegressor(random_state=42, n_estimators=n_estimators,\n",
    "                                      max_depth = max_depth,\n",
    "                                      min_samples_split=min_samples_split,\n",
    "                                      min_samples_leaf=min_samples_leaf)\n",
    "        model.fit(train[0],train[1].values.flatten())\n",
    "        preds = model.predict(val[0])\n",
    "        rmse.append(np.sqrt(np.mean(np.square(preds-val[1].values.flatten()))))\n",
    "        \n",
    "    if np.mean(rmse) < best_loss:\n",
    "        best_parameters = [n_estimators, max_depth, min_samples_split, min_samples_leaf]\n",
    "        best_loss = np.mean(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the results of randoom search\n",
    "print (\"Random Forest with RandomSearch\",\"\\n_\",\"\\nRMSE:\", best_loss ,\"\\n_\",\"\\nn_estimators:\", best_parameters[0], \n",
    "       \"\\nmax_depth:\", best_parameters[1],\"\\nmin_samples_split:\", best_parameters[2], \n",
    "       \"\\nmin_samples_leaf:\", best_parameters[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.4. Random Forest: hyper-parameter optimization (Bayesian Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a dictionary with hyperparameters to be tested\n",
    "bayesian_hyperparameters = {\"n_estimators\": (30,1000),\n",
    "                        \"max_depth\": (1,100),\n",
    "                        \"min_samples_split\": (2,5),\n",
    "                        \"min_samples_leaf\": (1,2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: rf_X is the list of features you created in the previous step\n",
    "# you need to define rf_X\n",
    "\n",
    "# define fuction to be maximised (negative RMSE)\n",
    "def BayesianForest(n_estimators,max_depth, min_samples_split,min_samples_leaf):\n",
    "        \n",
    "    rmse = []\n",
    "    for train, val in cross_val(rf_X,y,folds=7,random_state=42):\n",
    "        model = RandomForestRegressor(random_state=42, n_estimators=int(n_estimators),\n",
    "                                    max_depth=int(max_depth),\n",
    "                                    min_samples_split=int(min_samples_split),\n",
    "                                    min_samples_leaf=int(min_samples_leaf))\n",
    "    \n",
    "        model.fit(train[0],train[1].values.flatten())\n",
    "        preds = model.predict(val[0])\n",
    "        rmse.append(np.sqrt(np.mean(np.square(preds-val[1].values.flatten()))))\n",
    "\n",
    "    return -np.mean(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define optimizer\n",
    "optimizer = BayesianOptimization(\n",
    "    f=BayesianForest,\n",
    "    pbounds=bayesian_hyperparameters,\n",
    "    random_state=42,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run optimizer !\n",
    "optimizer.maximize(init_points=200, n_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print optimizer results\n",
    "print(optimizer.max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Extreme Gradent Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1. Extreme Gradent Boosting: feature selection testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_results=[]\n",
    "for train, val in cross_val(X,y,folds=7,random_state=42):\n",
    "    results=[]\n",
    "    for index in index_list:\n",
    "        selected_features = []\n",
    "        method_results=[]\n",
    "        for feature in train[0].columns[index[::-1]]:\n",
    "            selected_features.append(feature)\n",
    "            \n",
    "            model = xgboost.XGBRegressor(random_state=42)\n",
    "            model.fit(train[0][selected_features], train[1].values.flatten(),\n",
    "                      eval_metric='rmse', verbose=True)\n",
    "            \n",
    "            preds = model.predict(val[0][selected_features])\n",
    "            rmse = np.sqrt(np.mean(np.square(preds-val[1].values.flatten())))\n",
    "\n",
    "            method_results.append(rmse)\n",
    "        results.append(method_results)\n",
    "    fold_results.append(results)\n",
    "    \n",
    "results = np.mean(fold_results,axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2)=plt.subplots(1,2,figsize=(13,5))\n",
    "\n",
    "winner = np.argwhere(results == np.min(results)).flatten()[0]\n",
    "\n",
    "for i in range(4):\n",
    "    colors = [\"#40d491ff\",\"#ffe24dff\",\"#ff7f4dff\",\"#397abdff\"]\n",
    "    ax1.plot(results[i],label=names[i], color=colors[i])\n",
    "ax1.hlines(np.min(results),xmin=0,xmax=63,linestyle='--',color='#25521988',label='min RMSE')\n",
    "ax1.set_title(f'Regressor: XGB baseline \\nMinimum RMSE: {np.round(np.min(results),2)} t/ha \\n Method: {names[winner]}')\n",
    "ax1.set_ylabel('RMSE (t/ha)')\n",
    "ax1.set_xlabel('Number of features')\n",
    "\n",
    "\n",
    "ax1.legend(loc='upper right')\n",
    "\n",
    "for i in range(4):\n",
    "    colors = [\"#40d491ff\",\"#ffe24dff\",\"#ff7f4dff\",\"#397abdff\"]\n",
    "    ax2.plot(results[i],label=names[i], color=colors[i])\n",
    "ax2.hlines(np.min(results),xmin=0,xmax=63,linestyle='--',color='#25521988',label='Min RMSE')\n",
    "ax2.set_title(f'Regressor: XGB baseline \\nMinimum RMSE: {np.round(np.min(results),2)} t/ha \\n Method: {names[winner]}')\n",
    "ax2.set_xlim([12,30])\n",
    "ax2.set_ylim([45,49])\n",
    "ax2.set_ylabel('RMSE (t/ha)')\n",
    "ax2.set_xlabel('Number of features')\n",
    "\n",
    "ax2.legend(loc='upper right')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **IMPORTANT :**   \n",
    "Create a list of the best performing features. Such that, if the best perfoming XGB model was achieved by using the first 15 features selected by the MDA feature selection method, create a list with such fueatures.\n",
    "\n",
    "i.e. add a cell such that\n",
    "\n",
    "xgb_X = X [ [ ' feature1 ' , ' feature2 ' , ' feature3 ' , ... , ' featureN ' ] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2. Extreme Gradent Boosting (XGB): hyper-parameter optimization (Random Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a dictionary with hyperparameters to be tested\n",
    "random_hyperparameters = dict(alpha = np.arange(0,0.9), gamma = np.arange(0,0.9),\n",
    "                              learning_rate = np.arange(0,0.9), n_estimators = np.arange(30,1000),\n",
    "                              max_depth = np.arange(1,8), subsample = np.arange(0,0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: xgb_X is the list of features you created in the previous step\n",
    "# you need to define xgb_X\n",
    "\n",
    "n_iterations = 200\n",
    "best_loss = 1000\n",
    "for _ in range(n_iterations):\n",
    "    \n",
    "    alpha = np.random.choice(random_hyperparameters['alpha'],replace=True)\n",
    "    gamma=np.random.choice(random_hyperparameters['gamma'],replace=True)\n",
    "    learning_rate = np.random.choice(random_hyperparameters['learning_rate'],replace=True)\n",
    "    n_estimators = np.random.choice(random_hyperparameters['n_estimators'],replace=True)\n",
    "    max_depth=np.random.choice(random_hyperparameters['max_depth'],replace=True)\n",
    "    subsample = np.random.choice(random_hyperparameters['subsample'],replace=True)\n",
    "    \n",
    "    rmse = []\n",
    "    for train, val in cross_val(xgb_X,y,folds=7,random_state=42):\n",
    "        \n",
    "        model_random = xgboost.XGBRegressor(alpha = int(alpha), max_depth = int(max_depth),\n",
    "              gamma = gamma, n_estimators= int(n_estimators), learning_rate=learning_rate, \n",
    "                                     subsample = subsample, eval_metric = 'rmse', random_state=42)        \n",
    "        model_random.fit(train[0], train[1].values.flatten(), eval_metric='rmse', \n",
    "                  verbose=True)            \n",
    "        preds = model_random.predict(val[0])\n",
    "        rmse.append(np.sqrt(np.mean(np.square(preds-val[1].values.flatten()))))\n",
    "        \n",
    "        \n",
    "    if np.mean(rmse) < best_loss:\n",
    "        best_parameters = [alpha, gamma, learning_rate, n_estimators, max_depth, subsample ]\n",
    "        best_loss = np.mean(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print best hyperparameters results\n",
    "print (\"XGradientBoosting with RandomSearch\",\"\\n_\",\"\\nRMSE:\", best_loss ,\"\\n_\",\"\\nalpha:\", best_parameters[0], \n",
    "       \"\\ngamma:\", best_parameters[1],\"\\nlearning_rate:\", best_parameters[2], \n",
    "       \"\\nn_estimators:\", best_parameters[3], \"\\nmax_depth:\", best_parameters[4], \"\\nsubsample\", best_parameters[5] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.3. Extreme Gradient Boosting (XGB): hyper-parameter optimization (Bayesian Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a dictionary with hyperparameters to be tested\n",
    "hy_params = {\"alpha\": (0,0.9), 'subsample': (0,1),'gamma': (0, 0.9),'learning_rate':(0,0.9),'n_estimators':(30,1000), \"max_depth\": (1,8)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: xgb_X is the list of features you created in the previous step\n",
    "# you need to define xgb_X\n",
    "\n",
    "# define function to be maximized (negative RMSE)\n",
    "def BayesianForest(max_depth, gamma, n_estimators ,learning_rate, alpha, subsample):\n",
    "    rmse = []\n",
    "    for train, val in cross_val(xgb_X,y,folds=7):\n",
    "        \n",
    "        model = xgboost.XGBRegressor(alpha = int(alpha), max_depth = int(max_depth),\n",
    "              gamma = gamma,\n",
    "              n_estimators= int(n_estimators),\n",
    "              learning_rate=learning_rate, subsample = subsample,\n",
    "              eval_metric = 'rmse', random_state=42)\n",
    "        \n",
    "        model.fit(train[0], train[1].values.flatten(), eval_metric='rmse', \n",
    "                  verbose=True)\n",
    "            \n",
    "        preds = model.predict(val[0])\n",
    "        rmse.append(np.sqrt(np.mean(np.square(preds-val[1].values.flatten()))))\n",
    "    \n",
    "    return -np.mean(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define optimizer\n",
    "optimizer = BayesianOptimization(\n",
    "    f=BayesianForest, pbounds=hy_params\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run it\n",
    "optimizer.maximize(init_points=150, n_iter=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print hyperparameters that maximise the -RMSE\n",
    "print(optimizer.max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Linear Neural Network (LNN): feature selection testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_results=[]\n",
    "for train, val in cross_val(X,y,folds=7,random_state=42):\n",
    "    results=[]\n",
    "    for index in index_list:\n",
    "        selected_features = []\n",
    "        method_results=[]\n",
    "        for feature in train[0].columns[index[::-1]]:\n",
    "            selected_features.append(feature)\n",
    "            \n",
    "            linear = Sequential()\n",
    "            linear.add(Dense(units=1,activation='linear',  dtype='float64'))\n",
    "            linear.compile(optimizer='adam', loss='mse')\n",
    "            \n",
    "            linear.fit(train[0][selected_features], train[1].values.flatten(), epochs = 500, verbose=0)\n",
    "            linear_preds = linear.predict(val[0][selected_features])\n",
    "            rmse = np.sqrt(np.mean(np.square(linear_preds-val[1].values.flatten())))\n",
    "\n",
    "            method_results.append(rmse)\n",
    "        results.append(method_results)\n",
    "    fold_results.append(results)\n",
    "    \n",
    "results = np.mean(fold_results,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2)=plt.subplots(1,2,figsize=(13,5))\n",
    "\n",
    "winner = np.argwhere(results == np.min(results)).flatten()[0]\n",
    "\n",
    "for i in range(4):\n",
    "    colors = [\"#40d491ff\",\"#ffe24dff\",\"#ff7f4dff\",\"#397abdff\"]\n",
    "    ax1.plot(results[i],label=names[i], color=colors[i])\n",
    "ax1.hlines(np.min(results),xmin=0,xmax=63,linestyle='--',color='#25521988',label='min RMSE')\n",
    "ax1.set_title(f'ANN (linear) \\nMinimum RMSE: {np.round(np.min(results),2)}t/ha \\n Method: {names[winner]}')\n",
    "ax1.set_ylabel('RMSE (t/ha)')\n",
    "ax1.set_xlabel('Number of features')\n",
    "\n",
    "ax1.legend(loc='upper right')\n",
    "\n",
    "for i in range(4):\n",
    "    colors = [\"#40d491ff\",\"#ffe24dff\",\"#ff7f4dff\",\"#397abdff\"]\n",
    "    ax2.plot(results[i],label=names[i], color=colors[i])\n",
    "ax2.hlines(np.min(results),xmin=0,xmax=63,linestyle='--',color='#25521988',label='min RMSE')\n",
    "ax2.set_title(f'ANN (linear) \\nMinimum RMSE: {np.round(np.min(results),2)}t/ha \\n Method: {names[winner]}')\n",
    "ax2.set_ylabel('RMSE (t/ha)')\n",
    "ax2.set_xlabel('Number of features')\n",
    "ax2.set_xlim([30,50])\n",
    "ax2.set_ylim([75,80])\n",
    "\n",
    "ax2.legend(loc='upper right')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. Shallow Neural Network (SNN): feature selection testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_fold_results=[]\n",
    "for train, val in cross_val(X,y,folds=7,random_state=42):\n",
    "    simple_results=[]\n",
    "    for index in index_list:\n",
    "        selected_features = []\n",
    "        simple_method_results=[]\n",
    "        for feature in train[0].columns[index[::-1]]:\n",
    "            selected_features.append(feature)\n",
    "                     \n",
    "            simple = Sequential()\n",
    "            simple.add(Dense(units=8,activation='relu',  dtype='float64'))\n",
    "            simple.add(Dense(units=4,activation='relu',  dtype='float64'))\n",
    "            simple.add(Dense(units=1,activation='linear',  dtype='float64'))\n",
    "            \n",
    "            simple.compile(loss='mse',optimizer='adam')\n",
    "            \n",
    "            simple.fit(train[0][selected_features],train[1].values.flatten(),epochs=500, verbose=0)\n",
    "            \n",
    "            simple_preds = simple.predict(val[0][selected_features])\n",
    "            simple_rmse = np.sqrt(np.mean(np.square(simple_preds-val[1].values.flatten())))\n",
    "\n",
    "            \n",
    "            simple_method_results.append(simple_rmse)\n",
    "        simple_results.append(simple_method_results)\n",
    "    simple_fold_results.append(simple_results)\n",
    "    \n",
    "simple_results = np.mean(simple_fold_results,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2)=plt.subplots(1,2,figsize=(13,5))\n",
    "\n",
    "winner = np.argwhere(simple_results == np.min(simple_results)).flatten()[0]\n",
    "\n",
    "for i in range(4):\n",
    "    colors = [\"#40d491ff\",\"#ffe24dff\",\"#ff7f4dff\",\"#397abdff\"]\n",
    "    ax1.plot(simple_results[i],label=names[i], color=colors[i])\n",
    "ax1.hlines(np.min(simple_results),xmin=0,xmax=63,linestyle='--',color='#25521988',label='min RMSE')\n",
    "ax1.set_title(f'ANN (simple) \\nMinimum RMSE: {np.round(np.min(simple_results),2)}t/ha \\n Method: {names[winner]}')\n",
    "ax1.set_ylabel('RMSE (t/ha)')\n",
    "ax1.set_xlabel('Number of features')\n",
    "\n",
    "\n",
    "ax1.legend(loc='upper right')\n",
    "\n",
    "for i in range(4):\n",
    "    colors = [\"#40d491ff\",\"#ffe24dff\",\"#ff7f4dff\",\"#397abdff\"]\n",
    "    ax2.plot(simple_results[i],label=names[i], color=colors[i])\n",
    "ax2.hlines(np.min(simple_results),xmin=0,xmax=63,linestyle='--',color='#25521988',label='min RMSE')\n",
    "ax2.set_title(f'ANN (simple) \\nMinimum RMSE: {np.round(np.min(simple_results),2)}t/ha \\n Method: {names[winner]}')\n",
    "ax2.set_ylabel('RMSE (t/ha)')\n",
    "ax2.set_xlabel('Number of features')\n",
    "ax2.set_xlim([0,30])\n",
    "ax2.set_ylim([41,50])\n",
    "\n",
    "ax2.legend(loc='upper right')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6. Deep Neural Network (DNN): feature selection testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plex_fold_results=[]\n",
    "for train, val in cross_val(X,y,folds=7,random_state=42):\n",
    "    plex_results=[]\n",
    "    for index in index_list:\n",
    "        selected_features = []\n",
    "        plex_method_results=[]\n",
    "        for feature in train[0].columns[index[::-1]]:\n",
    "            selected_features.append(feature)\n",
    "            plex = Sequential()\n",
    "            plex.add(Dense(units=128,activation='relu',  dtype='float64'))\n",
    "            plex.add(Dropout(0.3))\n",
    "            plex.add(Dense(units=64,activation='relu',  dtype='float64'))\n",
    "            plex.add(Dense(units=32,activation='relu',  dtype='float64'))\n",
    "            plex.add(Dropout(0.1))\n",
    "            plex.add(Dense(units=16,activation='relu',  dtype='float64'))\n",
    "            plex.add(Dense(units=8,activation='relu',  dtype='float64'))\n",
    "            plex.add(Dense(units=1,activation='linear',  dtype='float64'))\n",
    "\n",
    "            plex.compile(loss='mse', optimizer='adam') \n",
    "\n",
    "            plex.fit(train[0][selected_features],train[1].values.flatten(),epochs=500, verbose=0)\n",
    "            \n",
    "            plex_preds = plex.predict(val[0][selected_features])\n",
    "            plex_rmse = np.sqrt(np.mean(np.square(plex_preds-val[1].values.flatten())))\n",
    "            \n",
    "            plex_method_results.append(plex_rmse)\n",
    "        plex_results.append(plex_method_results)\n",
    "    plex_fold_results.append(plex_results)\n",
    "    \n",
    "plex_results = np.mean(plex_fold_results,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2)=plt.subplots(1,2,figsize=(13,5))\n",
    "\n",
    "winner = np.argwhere(plex_results == np.min(plex_results)).flatten()[0]\n",
    "\n",
    "for i in range(4):\n",
    "    colors = [\"#40d491ff\",\"#ffe24dff\",\"#ff7f4dff\",\"#397abdff\"]\n",
    "    ax1.plot(plex_results[i],label=names[i], color=colors[i])\n",
    "ax1.hlines(np.min(plex_results),xmin=0,xmax=63,linestyle='--',color='#25521988',label='min RMSE')\n",
    "ax1.set_title(f'ANN (complex) \\nMinimum RMSE: {np.round(np.min(plex_results),2)}t/ha \\n Method: {names[winner]}')\n",
    "ax1.set_ylabel('RMSE (t/ha)')\n",
    "ax1.set_xlabel('Number of features')\n",
    "\n",
    "ax1.legend(loc='upper right')\n",
    "\n",
    "for i in range(4):\n",
    "    colors = [\"#40d491ff\",\"#ffe24dff\",\"#ff7f4dff\",\"#397abdff\"]\n",
    "    ax2.plot(plex_results[i],label=names[i], color=colors[i])\n",
    "ax2.hlines(np.min(plex_results),xmin=0,xmax=63,linestyle='--',color='#25521988',label='min RMSE')\n",
    "ax2.set_title(f'ANN (complex) \\nMinimum RMSE: {np.round(np.min(plex_results),2)}t/ha \\n Method: {names[winner]}')\n",
    "ax2.set_ylabel('RMSE (t/ha)')\n",
    "ax2.set_xlabel('Number of features')\n",
    "ax2.set_xlim([0,10])\n",
    "ax2.set_ylim([41,48])\n",
    "\n",
    "ax2.legend(loc='upper right')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Goodness of fit\n",
    "\n",
    "The following cells address goodness of fit of the Extreme Gradent Boosting (XGB) predictions. Add new cells to assess the goodness of fit of the other models predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each fold, put predictions and true measures of the XGB into arrays \n",
    "true_array = np.asarray(true, dtype=object)\n",
    "preds_array = np.asarray(pred, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save prediction and true values to folder\n",
    "np.savetxt(\"predictions/true_fold_1.csv\", true_array[0])\n",
    "np.savetxt(\"predictions/true_fold_2.csv\", true_array[1])\n",
    "np.savetxt(\"predictions/true_fold_3.csv\", true_array[2])\n",
    "np.savetxt(\"predictions/true_fold_4.csv\", true_array[3])\n",
    "np.savetxt(\"predictions/true_fold_5.csv\", true_array[4])\n",
    "np.savetxt(\"predictions/true_fold_6.csv\", true_array[5])\n",
    "np.savetxt(\"predictions/true_fold_7.csv\", true_array[6])\n",
    "\n",
    "np.savetxt(\"predictions/pred_fold_1.csv\", preds_array[0])\n",
    "np.savetxt(\"predictions/pred_fold_2.csv\", preds_array[1])\n",
    "np.savetxt(\"predictions/pred_fold_3.csv\", preds_array[2])\n",
    "np.savetxt(\"predictions/pred_fold_4.csv\", preds_array[3])\n",
    "np.savetxt(\"predictions/pred_fold_5.csv\", preds_array[4])\n",
    "np.savetxt(\"predictions/pred_fold_6.csv\", preds_array[5])\n",
    "np.savetxt(\"predictions/pred_fold_7.csv\", preds_array[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the predictions and true values\n",
    "true_fold_1 = pd.read_csv(\"predictions/true_fold_1.csv\")\n",
    "pred_fold_1 = pd.read_csv(\"predictions/pred_fold_1.csv\")\n",
    "true_fold_2 = pd.read_csv(\"predictions/true_fold_2.csv\")\n",
    "pred_fold_2 = pd.read_csv(\"predictions/pred_fold_2.csv\")\n",
    "true_fold_3 = pd.read_csv(\"predictions/true_fold_3.csv\")\n",
    "pred_fold_3 = pd.read_csv(\"predictions/pred_fold_3.csv\")\n",
    "true_fold_4 = pd.read_csv(\"predictions/true_fold_4.csv\")\n",
    "pred_fold_4 = pd.read_csv(\"predictions/pred_fold_4.csv\")\n",
    "true_fold_5 = pd.read_csv(\"predictions/true_fold_5.csv\")\n",
    "pred_fold_5 = pd.read_csv(\"predictions/pred_fold_5.csv\")\n",
    "true_fold_6 = pd.read_csv(\"predictions/true_fold_6.csv\")\n",
    "pred_fold_6 = pd.read_csv(\"predictions/pred_fold_6.csv\")\n",
    "true_fold_7 = pd.read_csv(\"predictions/true_fold_7.csv\")\n",
    "pred_fold_7 = pd.read_csv(\"predictions/pred_fold_7.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot measured and predicted values\n",
    "plt.scatter(true_fold_1, pred_fold_1, label='Fold 1', color= '#FD3838')\n",
    "plt.scatter(true_fold_2, pred_fold_2, label='Fold 2', color= '#F98B2B')\n",
    "plt.scatter(true_fold_3, pred_fold_3, label='Fold 3', color= '#F3E438')\n",
    "plt.scatter(true_fold_4, pred_fold_4, label='Fold 4', color= '#84F936')\n",
    "plt.scatter(true_fold_5, pred_fold_5, label='Fold 5', color='#4BF1D8')\n",
    "plt.scatter(true_fold_6, pred_fold_6, label='Fold 6', color='#5D79F4')\n",
    "plt.scatter(true_fold_7, pred_fold_7, label='Fold 7', color='#FF8BD6')\n",
    "\n",
    "\n",
    "plt.axline([-1, -1], [1, 1], linestyle='--', color='#25521988', label='x = y')\n",
    "\n",
    "plt.xlabel('Measured AGB (t/ha)')\n",
    "plt.ylabel('Predicted AGB (t/ha)')\n",
    "plt.axis('square')\n",
    "plt.title('XGB (baseline) \\nGoodness of fit on unseen data \\nMeasured vs Predicted')\n",
    "plt.legend(bbox_to_anchor=(1, 1.03))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Explanation with SHAP\n",
    "Ideally, a Machine Learning (ML) model should be highly accurate and simple to interpret. By ”simple to interpret” is intended the ability to expose its performance in an understandable and intuitive way. Unfortunately, the more the model complexity increases, the harder it is to understand how certain values were predicted and which features had contributed more to those predictions. When working with a simple model, the impact that a feature has on the model output is easily interpretable by looking at its weights; whereas, complex models such as ensemble methods or deep networks are not as easy to understand; in this scenario, a model explainer can help interpreting the model results.\n",
    "\n",
    "In the following cells, the SHapley Additive exPlanations (SHAP) package was used as model explainer. SHAP was created by Lundberg et al. (2020) and, according to its author, its implementation allows for appropriate user trust, provides insights for model improvement, and supports the understanding of the problem being modelled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a model, or use one of the previously created models\n",
    "# in this example we initialize an XGB model\n",
    "model_xgb = xgboost.XGBRegressor(random_state=42)    \n",
    "model_xgb.fit(xgb_X, y.values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer_xgb = shap.TreeExplainer(model_xgb)\n",
    "shap_values_xgb = explainer_xgb.shap_values(xgb_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize plot which shoes the effects of all the features on model output\n",
    "shap.summary_plot(shap_values_xgb, xgb_X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
